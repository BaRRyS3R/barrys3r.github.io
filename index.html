<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Face & Emotion Tracking</title>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: #000;
    }
    video, canvas {
      position: absolute;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
    }
    #emotion {
      position: absolute;
      top: 10px;
      left: 50%;
      transform: translateX(-50%);
      font-size: 2rem;
      color: white;
      font-family: Arial, sans-serif;
      text-shadow: 0 0 10px black;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>
  <div id="emotion">Определение эмоций...</div>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const emotionText = document.getElementById('emotion');
    const ctx = canvas.getContext('2d');

    // Запрашиваем камеру
    async function startCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
    }

    // Загружаем модели face-api.js
    async function loadModels() {
      await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');
      await faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');
      await faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');
    }

    // Запуск анализа
    video.addEventListener('play', async () => {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;

      async function detect() {
        const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceExpressions();

        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        if (detections.length > 0) {
          const expressions = detections[0].expressions;
          let maxEmotion = '';
          let maxValue = 0;
          for (let [emotion, value] of Object.entries(expressions)) {
            if (value > maxValue) {
              maxValue = value;
              maxEmotion = emotion;
            }
          }

          emotionText.textContent = `Эмоция: ${translateEmotion(maxEmotion)} (${(maxValue*100).toFixed(1)}%)`;

          const box = detections[0].detection.box;
          ctx.strokeStyle = 'lime';
          ctx.lineWidth = 3;
          ctx.strokeRect(box.x, box.y, box.width, box.height);
        } else {
          emotionText.textContent = "Лицо не обнаружено";
        }

        requestAnimationFrame(detect);
      }
      detect();
    });

    function translateEmotion(emotion) {
      const map = {
        neutral: "Нейтральное",
        happy: "Счастливое",
        sad: "Грустное",
        angry: "Злое",
        fearful: "Испуганное",
        disgusted: "Отвращение",
        surprised: "Удивлённое"
      };
      return map[emotion] || emotion;
    }

    loadModels().then(startCamera);
  </script>
</body>
</html>
