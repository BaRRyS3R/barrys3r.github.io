<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Face & Emotion Tracking</title>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: #000;
    }
    video, canvas {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
      z-index: 0;
    }
    #emotion {
      position: fixed;
      top: 20px;
      left: 50%;
      transform: translateX(-50%);
      font-size: 2rem;
      color: white;
      font-family: Arial, sans-serif;
      text-shadow: 0 0 10px black;
      z-index: 10;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>
  <div id="emotion">Загрузка...</div>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const emotionText = document.getElementById('emotion');
    const ctx = canvas.getContext('2d');

    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: {
            facingMode: 'user', // фронтальная камера
            width: { ideal: 1280 },
            height: { ideal: 720 }
          },
          audio: false
        });
        video.srcObject = stream;
        await video.play();
      } catch (err) {
        console.error("Ошибка доступа к камере:", err);
        emotionText.textContent = "Не удалось получить доступ к камере";
      }
    }

    async function loadModels() {
      const modelUrl = "https://cdn.jsdelivr.net/npm/face-api.js/models";
      await faceapi.nets.tinyFaceDetector.loadFromUri(modelUrl);
      await faceapi.nets.faceLandmark68Net.loadFromUri(modelUrl);
      await faceapi.nets.faceExpressionNet.loadFromUri(modelUrl);
    }

    function translateEmotion(emotion) {
      const map = {
        neutral: "Нейтральное",
        happy: "Счастливое",
        sad: "Грустное",
        angry: "Злое",
        fearful: "Испуганное",
        disgusted: "Отвращение",
        surprised: "Удивлённое"
      };
      return map[emotion] || emotion;
    }

    video.addEventListener('loadedmetadata', () => {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;

      async function detect() {
        const detections = await faceapi.detectAllFaces(
          video, new faceapi.TinyFaceDetectorOptions()
        ).withFaceLandmarks().withFaceExpressions();

        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        if (detections.length > 0) {
          const best = detections[0];
          const box = best.detection.box;

          // Найдём эмоцию с макс. вероятностью
          let maxEmotion = '';
          let maxValue = 0;
          for (let [emo, val] of Object.entries(best.expressions)) {
            if (val > maxValue) {
              maxValue = val;
              maxEmotion = emo;
            }
          }

          emotionText.textContent = `${translateEmotion(maxEmotion)} (${(maxValue * 100).toFixed(0)}%)`;

          // Рисуем рамку
          ctx.strokeStyle = "lime";
          ctx.lineWidth = 3;
          ctx.strokeRect(box.x, box.y, box.width, box.height);
        } else {
          emotionText.textContent = "Лицо не найдено";
        }
        requestAnimationFrame(detect);
      }
      detect();
    });

    loadModels().then(startCamera);
  </script>
</body>
</html>
